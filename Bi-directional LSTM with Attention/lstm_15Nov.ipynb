{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "SZGqhNMgY7jE"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import sys\n",
        "import re\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import random as rn\n",
        "import keras\n",
        "import os\n",
        "from keras.layers import Embedding, LSTM, Conv1D, BatchNormalization, Multiply, Permute, Dot\n",
        "from keras.layers import Dropout, MaxPooling1D, GlobalMaxPooling1D, Lambda, RepeatVector\n",
        "from keras.layers import Input, Activation, Bidirectional, GRU, Dense\n",
        "#from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import pad_sequences\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Model\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras import backend as K\n",
        "from sklearn.metrics import f1_score, confusion_matrix, mean_squared_error\n",
        "np.random.seed(42)\n",
        "rn.seed(12345)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BASE_DIR = ''\n",
        "GLOVE_DIR = os.path.join(BASE_DIR, '')\n",
        "MAX_SEQUENCE_LENGTH = 100  # max input sequence length\n",
        "EMBEDDING_DIM = 200  # word embedding size\n",
        "VALIDATION_SPLIT = 0.2  # ratio of validation data"
      ],
      "metadata": {
        "id": "T0r_Vd8Kcdik"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def RNN_model(input_layer, num_class):\n",
        "    def smoothing_attention(x):\n",
        "        e = K.sigmoid(x)\n",
        "        s = K.sum(e, axis=-1, keepdims=True)\n",
        "        return e / s\n",
        "\n",
        "    reg = 0.0001\n",
        "    dropout = 0.5\n",
        "    hidden_dim = 1024\n",
        "\n",
        "    vector = Bidirectional(LSTM(hidden_dim, return_sequences=False, kernel_regularizer=keras.regularizers.l2(reg)))(input_layer)\n",
        "    lstm = Bidirectional(LSTM(hidden_dim, return_sequences=True, kernel_regularizer=keras.regularizers.l2(reg)))(input_layer)\n",
        "    ee = Dot(axes=-1, normalize=True)([vector, lstm])\n",
        "    weights = Lambda(smoothing_attention)(ee)\n",
        "    weights = RepeatVector(2*hidden_dim)(weights)\n",
        "    weights = Permute((2, 1))(weights)\n",
        "    output = Multiply()([weights, lstm])\n",
        "    output = Lambda(lambda x: K.sum(x, axis=1))(output)\n",
        "    output = Dense(512)(output)\n",
        "    output = BatchNormalization()(output)\n",
        "    output = Activation(\"relu\")(output)\n",
        "    output = Dense(256)(output)\n",
        "    output = BatchNormalization()(output)\n",
        "    output = Activation(\"relu\")(output)\n",
        "    output = Dropout(dropout)(output)\n",
        "    output = Dense(num_class, activation='softmax')(output)\n",
        "    model = Model(sequence_input, output)\n",
        "    print(model.summary())\n",
        "    return model"
      ],
      "metadata": {
        "id": "WnAXCsuwcY2s"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount = True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DA3AjBEXdYWc",
        "outputId": "6b3c3073-4e2b-4cb6-ba68-289c6612cea9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    embeddings_index = {}\n",
        "    #with open(os.path.join(GLOVE_DIR, 'glove.42B.300d.txt')) as f:\n",
        "    #with open(os.path.join('glove.6B.50d.txt'), encoding=\"utf-8\", errors=\"ignore\") as f:\n",
        "    with open('/content/drive/My Drive/Colab Notebooks/NLP_FS23_proj/glove.6B.200d.txt', encoding=\"utf-8\", errors=\"ignore\") as f:\n",
        "        for line in f:\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            coefs = np.asarray(values[1:], dtype='float32')\n",
        "            embeddings_index[word] = coefs\n",
        "    print('Found %s word vectors.' % len(embeddings_index))\n",
        "\n",
        "    X_train = []\n",
        "    Y_train = []\n",
        "    label_to_y = dict()\n",
        "    with open(\"/content/drive/My Drive/Colab Notebooks/NLP_FS23_proj/TRAIN_FILE.txt\") as f:\n",
        "        for idx, l in enumerate(f):\n",
        "            l = l.strip()\n",
        "            if idx % 4 == 0:\n",
        "                ID, sentence = l.split(\"\\t\")\n",
        "                sentence = sentence[1:-1]\n",
        "                sentence = sentence.replace('<e1>', 'xxxxxxxxxe1xxxxxxxxx ')\n",
        "                sentence = sentence.replace('<e2>', 'xxxxxxxxxe2xxxxxxxxx ')\n",
        "                sentence = sentence.replace('</e1>', ' ssssssssse1sssssssss')\n",
        "                sentence = sentence.replace('</e2>', ' ssssssssse2sssssssss')\n",
        "                X_train.append(sentence)\n",
        "            elif idx % 4 == 1:\n",
        "                label = l\n",
        "                if label not in label_to_y:\n",
        "                    label_to_y[label] = len(label_to_y)\n",
        "                Y_train.append(label_to_y[label])\n",
        "            else:\n",
        "                pass\n",
        "\n",
        "    y_to_label = {j: i for i, j in label_to_y.items()}\n",
        "    Y_train = np.array(Y_train, dtype=int)\n",
        "    num_class = max(Y_train) + 1\n",
        "    Y_train = to_categorical(Y_train)\n",
        "    tokenizer = Tokenizer(oov_token=\"UNK\")\n",
        "    tokenizer.fit_on_texts(X_train)\n",
        "    sequences = tokenizer.texts_to_sequences(X_train)\n",
        "    X_train = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "    print('Preparing embedding matrix.')\n",
        "\n",
        "    if VALIDATION_SPLIT > 0:\n",
        "        indices = np.arange(len(Y_train))\n",
        "        np.random.shuffle(indices)\n",
        "        val_index = int(VALIDATION_SPLIT * len(Y_train))\n",
        "        X_val = X_train[indices[:val_index]]\n",
        "        Y_val = Y_train[indices[:val_index]]\n",
        "        X_train = X_train[indices[val_index:]]\n",
        "        Y_train = Y_train[indices[val_index:]]\n",
        "\n",
        "    X_test = []\n",
        "    ID_test = []\n",
        "    with open(\"/content/drive/My Drive/Colab Notebooks/NLP_FS23_proj/TEST_FILE.txt\") as f:\n",
        "        for l in f:\n",
        "            ID, sentence = l.strip().split(\"\\t\")\n",
        "            sentence = sentence[1:-1]\n",
        "            sentence = sentence.replace('<e1>', 'xxxxxxxxxe1xxxxxxxxx ')\n",
        "            sentence = sentence.replace('<e2>', 'xxxxxxxxxe2xxxxxxxxx ')\n",
        "            sentence = sentence.replace('</e1>', ' ssssssssse1sssssssss')\n",
        "            sentence = sentence.replace('</e2>', ' ssssssssse2sssssssss')\n",
        "            ID_test.append(ID)\n",
        "            X_test.append(sentence)\n",
        "    sequences = tokenizer.texts_to_sequences(X_test)\n",
        "    X_test = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "    print('Preparing embedding matrix. - 2')\n",
        "\n",
        "    word_index = tokenizer.word_index\n",
        "    num_words = len(word_index) + 1\n",
        "    embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
        "    for word, i in word_index.items():\n",
        "        embedding_vector = embeddings_index.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            embedding_matrix[i] = embedding_vector\n",
        "    embedding_layer = Embedding(num_words,\n",
        "                                EMBEDDING_DIM,\n",
        "                                weights=[embedding_matrix],\n",
        "                                input_length=MAX_SEQUENCE_LENGTH,\n",
        "                                trainable=True)\n",
        "    sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH, ), dtype='int32')\n",
        "    embedded_sequences = embedding_layer(sequence_input)\n",
        "\n",
        "    print(\"Start training...\")\n",
        "    model = RNN_model(embedded_sequences, num_class)\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=keras.optimizers.Adam(learning_rate=0.001, clipvalue=15), metrics=['accuracy'])\n",
        "    early_stop = EarlyStopping(monitor='val_loss' if VALIDATION_SPLIT > 0 else \"loss\", patience=15, mode='min')\n",
        "    model.fit(X_train, Y_train, batch_size=128, epochs=50, callbacks=[early_stop], validation_data=(X_val, Y_val) if VALIDATION_SPLIT > 0 else None)\n",
        "    Y_pre = model.predict(X_test)\n",
        "    Y_pre = np.argmax(Y_pre, axis=1)\n",
        "    Y_pre = [y_to_label[i] for i in Y_pre]\n",
        "    with open(\"/content/drive/My Drive/Colab Notebooks/NLP_FS23_proj/predict.txt\", 'w') as f:\n",
        "        for ID, label in zip(ID_test, Y_pre):\n",
        "            f.write(ID + \"\\t\" + label + \"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XS2MOGv9cjAF",
        "outputId": "c55aa0f8-7548-4f3f-c598-da08622c29ea"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 400000 word vectors.\n",
            "Preparing embedding matrix.\n",
            "Preparing embedding matrix. - 2\n",
            "Start training...\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)        [(None, 100)]                0         []                            \n",
            "                                                                                                  \n",
            " embedding (Embedding)       (None, 100, 200)             3912600   ['input_1[0][0]']             \n",
            "                                                                                                  \n",
            " bidirectional (Bidirection  (None, 2048)                 1003520   ['embedding[0][0]']           \n",
            " al)                                                      0                                       \n",
            "                                                                                                  \n",
            " bidirectional_1 (Bidirecti  (None, 100, 2048)            1003520   ['embedding[0][0]']           \n",
            " onal)                                                    0                                       \n",
            "                                                                                                  \n",
            " dot (Dot)                   (None, 100)                  0         ['bidirectional[0][0]',       \n",
            "                                                                     'bidirectional_1[0][0]']     \n",
            "                                                                                                  \n",
            " lambda (Lambda)             (None, 100)                  0         ['dot[0][0]']                 \n",
            "                                                                                                  \n",
            " repeat_vector (RepeatVecto  (None, 2048, 100)            0         ['lambda[0][0]']              \n",
            " r)                                                                                               \n",
            "                                                                                                  \n",
            " permute (Permute)           (None, 100, 2048)            0         ['repeat_vector[0][0]']       \n",
            "                                                                                                  \n",
            " multiply (Multiply)         (None, 100, 2048)            0         ['permute[0][0]',             \n",
            "                                                                     'bidirectional_1[0][0]']     \n",
            "                                                                                                  \n",
            " lambda_1 (Lambda)           (None, 2048)                 0         ['multiply[0][0]']            \n",
            "                                                                                                  \n",
            " dense (Dense)               (None, 512)                  1049088   ['lambda_1[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization (Batch  (None, 512)                  2048      ['dense[0][0]']               \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation (Activation)     (None, 512)                  0         ['batch_normalization[0][0]'] \n",
            "                                                                                                  \n",
            " dense_1 (Dense)             (None, 256)                  131328    ['activation[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_1 (Bat  (None, 256)                  1024      ['dense_1[0][0]']             \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " activation_1 (Activation)   (None, 256)                  0         ['batch_normalization_1[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " dropout (Dropout)           (None, 256)                  0         ['activation_1[0][0]']        \n",
            "                                                                                                  \n",
            " dense_2 (Dense)             (None, 19)                   4883      ['dropout[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 25171371 (96.02 MB)\n",
            "Trainable params: 25169835 (96.02 MB)\n",
            "Non-trainable params: 1536 (6.00 KB)\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Epoch 1/50\n",
            "50/50 [==============================] - 46s 572ms/step - loss: 2.4083 - accuracy: 0.3069 - val_loss: 2.7539 - val_accuracy: 0.1944\n",
            "Epoch 2/50\n",
            "50/50 [==============================] - 26s 513ms/step - loss: 1.6513 - accuracy: 0.4933 - val_loss: 2.6005 - val_accuracy: 0.1756\n",
            "Epoch 3/50\n",
            "50/50 [==============================] - 27s 531ms/step - loss: 1.2277 - accuracy: 0.6237 - val_loss: 2.4393 - val_accuracy: 0.1794\n",
            "Epoch 4/50\n",
            "50/50 [==============================] - 27s 535ms/step - loss: 0.8779 - accuracy: 0.7377 - val_loss: 2.5029 - val_accuracy: 0.1737\n",
            "Epoch 5/50\n",
            "50/50 [==============================] - 27s 537ms/step - loss: 0.6349 - accuracy: 0.8198 - val_loss: 2.5456 - val_accuracy: 0.2006\n",
            "Epoch 6/50\n",
            "50/50 [==============================] - 27s 542ms/step - loss: 0.4476 - accuracy: 0.8853 - val_loss: 2.5224 - val_accuracy: 0.2056\n",
            "Epoch 7/50\n",
            "50/50 [==============================] - 27s 535ms/step - loss: 0.3073 - accuracy: 0.9292 - val_loss: 2.3934 - val_accuracy: 0.2706\n",
            "Epoch 8/50\n",
            "50/50 [==============================] - 27s 549ms/step - loss: 0.2065 - accuracy: 0.9609 - val_loss: 2.2519 - val_accuracy: 0.3200\n",
            "Epoch 9/50\n",
            "50/50 [==============================] - 27s 541ms/step - loss: 0.1608 - accuracy: 0.9712 - val_loss: 2.1661 - val_accuracy: 0.3994\n",
            "Epoch 10/50\n",
            "50/50 [==============================] - 27s 550ms/step - loss: 0.1158 - accuracy: 0.9883 - val_loss: 1.8940 - val_accuracy: 0.4700\n",
            "Epoch 11/50\n",
            "50/50 [==============================] - 27s 543ms/step - loss: 0.0894 - accuracy: 0.9930 - val_loss: 1.5154 - val_accuracy: 0.5981\n",
            "Epoch 12/50\n",
            "50/50 [==============================] - 28s 553ms/step - loss: 0.0743 - accuracy: 0.9961 - val_loss: 1.4981 - val_accuracy: 0.6119\n",
            "Epoch 13/50\n",
            "50/50 [==============================] - 28s 556ms/step - loss: 0.0686 - accuracy: 0.9947 - val_loss: 1.5186 - val_accuracy: 0.6169\n",
            "Epoch 14/50\n",
            "50/50 [==============================] - 28s 560ms/step - loss: 0.0629 - accuracy: 0.9956 - val_loss: 1.5626 - val_accuracy: 0.6456\n",
            "Epoch 15/50\n",
            "50/50 [==============================] - 27s 550ms/step - loss: 0.0696 - accuracy: 0.9919 - val_loss: 1.8083 - val_accuracy: 0.5756\n",
            "Epoch 16/50\n",
            "50/50 [==============================] - 28s 564ms/step - loss: 0.0714 - accuracy: 0.9919 - val_loss: 1.6342 - val_accuracy: 0.6344\n",
            "Epoch 17/50\n",
            "50/50 [==============================] - 27s 548ms/step - loss: 0.0710 - accuracy: 0.9920 - val_loss: 1.7843 - val_accuracy: 0.6012\n",
            "Epoch 18/50\n",
            "50/50 [==============================] - 28s 563ms/step - loss: 0.0598 - accuracy: 0.9959 - val_loss: 1.7753 - val_accuracy: 0.6325\n",
            "Epoch 19/50\n",
            "50/50 [==============================] - 27s 551ms/step - loss: 0.0520 - accuracy: 0.9966 - val_loss: 1.8007 - val_accuracy: 0.6187\n",
            "Epoch 20/50\n",
            "50/50 [==============================] - 28s 558ms/step - loss: 0.0431 - accuracy: 0.9992 - val_loss: 1.8152 - val_accuracy: 0.6237\n",
            "Epoch 21/50\n",
            "50/50 [==============================] - 28s 559ms/step - loss: 0.0421 - accuracy: 0.9983 - val_loss: 2.0617 - val_accuracy: 0.5987\n",
            "Epoch 22/50\n",
            "50/50 [==============================] - 27s 549ms/step - loss: 0.0445 - accuracy: 0.9978 - val_loss: 1.9133 - val_accuracy: 0.6263\n",
            "Epoch 23/50\n",
            "50/50 [==============================] - 28s 558ms/step - loss: 0.0675 - accuracy: 0.9903 - val_loss: 2.9849 - val_accuracy: 0.4762\n",
            "Epoch 24/50\n",
            "50/50 [==============================] - 28s 558ms/step - loss: 0.0792 - accuracy: 0.9872 - val_loss: 2.1249 - val_accuracy: 0.5412\n",
            "Epoch 25/50\n",
            "50/50 [==============================] - 28s 560ms/step - loss: 0.0661 - accuracy: 0.9925 - val_loss: 2.5550 - val_accuracy: 0.5756\n",
            "Epoch 26/50\n",
            "50/50 [==============================] - 28s 560ms/step - loss: 0.0621 - accuracy: 0.9939 - val_loss: 2.1438 - val_accuracy: 0.5950\n",
            "Epoch 27/50\n",
            "50/50 [==============================] - 28s 559ms/step - loss: 0.0498 - accuracy: 0.9966 - val_loss: 2.0316 - val_accuracy: 0.6288\n",
            "85/85 [==============================] - 6s 60ms/step\n"
          ]
        }
      ]
    }
  ]
}