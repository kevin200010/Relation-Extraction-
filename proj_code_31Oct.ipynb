{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyPFq/CpAb62SLbBNKFZccmi"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GGbuwnAVcmLs","executionInfo":{"status":"ok","timestamp":1698801307925,"user_tz":240,"elapsed":10333,"user":{"displayName":"sivaram learning","userId":"00911034967220677585"}},"outputId":"d66cac1c-b576-4f05-c388-d95177769735"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (2.14.0)\n"]}]},{"cell_type":"code","execution_count":7,"metadata":{"id":"SZGqhNMgY7jE","executionInfo":{"status":"ok","timestamp":1698802921780,"user_tz":240,"elapsed":323,"user":{"displayName":"sivaram learning","userId":"00911034967220677585"}}},"outputs":[],"source":["import json\n","import sys\n","import re\n","import numpy as np\n","import tensorflow as tf\n","import random as rn\n","import keras\n","import os\n","from keras.layers import Embedding, LSTM, Conv1D, BatchNormalization, Multiply, Permute, Dot\n","from keras.layers import Dropout, MaxPooling1D, GlobalMaxPooling1D, Lambda, RepeatVector\n","from keras.layers import Input, Activation, Bidirectional, GRU, Dense\n","#from keras.preprocessing.sequence import pad_sequences\n","from keras.utils import pad_sequences\n","from keras.preprocessing.text import Tokenizer\n","from keras.utils import to_categorical\n","from keras.models import Model\n","from keras.callbacks import EarlyStopping\n","from keras import backend as K\n","from sklearn.metrics import f1_score, confusion_matrix, mean_squared_error\n","np.random.seed(42)\n","rn.seed(12345)"]},{"cell_type":"code","source":["BASE_DIR = ''\n","GLOVE_DIR = os.path.join(BASE_DIR, '')\n","MAX_SEQUENCE_LENGTH = 100  # max input sequence length\n","EMBEDDING_DIM = 50  # word embedding size\n","VALIDATION_SPLIT = 0.1  # ratio of validation data"],"metadata":{"id":"T0r_Vd8Kcdik","executionInfo":{"status":"ok","timestamp":1698802927701,"user_tz":240,"elapsed":335,"user":{"displayName":"sivaram learning","userId":"00911034967220677585"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["def RNN_model(input_layer, num_class):\n","    def smoothing_attention(x):\n","        e = K.sigmoid(x)\n","        s = K.sum(e, axis=-1, keepdims=True)\n","        return e / s\n","\n","    reg = 0.0001\n","    dropout = 0.5\n","    hidden_dim = 1024\n","\n","    vector = Bidirectional(LSTM(hidden_dim, return_sequences=False, kernel_regularizer=keras.regularizers.l2(reg)))(input_layer)\n","    lstm = Bidirectional(LSTM(hidden_dim, return_sequences=True, kernel_regularizer=keras.regularizers.l2(reg)))(input_layer)\n","    ee = Dot(axes=-1, normalize=True)([vector, lstm])\n","    weights = Lambda(smoothing_attention)(ee)\n","    weights = RepeatVector(2*hidden_dim)(weights)\n","    weights = Permute((2, 1))(weights)\n","    output = Multiply()([weights, lstm])\n","    output = Lambda(lambda x: K.sum(x, axis=1))(output)\n","    output = Dense(512)(output)\n","    output = BatchNormalization()(output)\n","    output = Activation(\"relu\")(output)\n","    output = Dense(256)(output)\n","    output = BatchNormalization()(output)\n","    output = Activation(\"relu\")(output)\n","    output = Dropout(dropout)(output)\n","    output = Dense(num_class, activation='softmax')(output)\n","    model = Model(sequence_input, output)\n","    print(model.summary())\n","    return model"],"metadata":{"id":"WnAXCsuwcY2s","executionInfo":{"status":"ok","timestamp":1698802065902,"user_tz":240,"elapsed":502,"user":{"displayName":"sivaram learning","userId":"00911034967220677585"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount = True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DA3AjBEXdYWc","executionInfo":{"status":"ok","timestamp":1698802943783,"user_tz":240,"elapsed":3125,"user":{"displayName":"sivaram learning","userId":"00911034967220677585"}},"outputId":"09606b96-1df2-44ff-8758-f13d304fce67"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["if __name__ == \"__main__\":\n","    embeddings_index = {}\n","    #with open(os.path.join(GLOVE_DIR, 'glove.42B.300d.txt')) as f:\n","    #with open(os.path.join('glove.6B.50d.txt'), encoding=\"utf-8\", errors=\"ignore\") as f:\n","    with open('/content/drive/My Drive/Colab Notebooks/NLP_FS23_proj/glove.6B.50d.txt', encoding=\"utf-8\", errors=\"ignore\") as f:\n","        for line in f:\n","            values = line.split()\n","            word = values[0]\n","            coefs = np.asarray(values[1:], dtype='float32')\n","            embeddings_index[word] = coefs\n","    print('Found %s word vectors.' % len(embeddings_index))\n","\n","    X_train = []\n","    Y_train = []\n","    label_to_y = dict()\n","    with open(\"/content/drive/My Drive/Colab Notebooks/NLP_FS23_proj/TRAIN_FILE.txt\") as f:\n","        for idx, l in enumerate(f):\n","            l = l.strip()\n","            if idx % 4 == 0:\n","                ID, sentence = l.split(\"\\t\")\n","                sentence = sentence[1:-1]\n","                sentence = sentence.replace('<e1>', 'xxxxxxxxxe1xxxxxxxxx ')\n","                sentence = sentence.replace('<e2>', 'xxxxxxxxxe2xxxxxxxxx ')\n","                sentence = sentence.replace('</e1>', ' ssssssssse1sssssssss')\n","                sentence = sentence.replace('</e2>', ' ssssssssse2sssssssss')\n","                X_train.append(sentence)\n","            elif idx % 4 == 1:\n","                label = l\n","                if label not in label_to_y:\n","                    label_to_y[label] = len(label_to_y)\n","                Y_train.append(label_to_y[label])\n","            else:\n","                pass\n","\n","    y_to_label = {j: i for i, j in label_to_y.items()}\n","    Y_train = np.array(Y_train, dtype=int)\n","    num_class = max(Y_train) + 1\n","    Y_train = to_categorical(Y_train)\n","    tokenizer = Tokenizer(oov_token=\"UNK\")\n","    tokenizer.fit_on_texts(X_train)\n","    sequences = tokenizer.texts_to_sequences(X_train)\n","    X_train = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n","    print('Preparing embedding matrix.')\n","\n","    if VALIDATION_SPLIT > 0:\n","        indices = np.arange(len(Y_train))\n","        np.random.shuffle(indices)\n","        val_index = int(VALIDATION_SPLIT * len(Y_train))\n","        X_val = X_train[indices[:val_index]]\n","        Y_val = Y_train[indices[:val_index]]\n","        X_train = X_train[indices[val_index:]]\n","        Y_train = Y_train[indices[val_index:]]\n","\n","    X_test = []\n","    ID_test = []\n","    with open(\"/content/drive/My Drive/Colab Notebooks/NLP_FS23_proj/TEST_FILE.txt\") as f:\n","        for l in f:\n","            ID, sentence = l.strip().split(\"\\t\")\n","            sentence = sentence[1:-1]\n","            sentence = sentence.replace('<e1>', 'xxxxxxxxxe1xxxxxxxxx ')\n","            sentence = sentence.replace('<e2>', 'xxxxxxxxxe2xxxxxxxxx ')\n","            sentence = sentence.replace('</e1>', ' ssssssssse1sssssssss')\n","            sentence = sentence.replace('</e2>', ' ssssssssse2sssssssss')\n","            ID_test.append(ID)\n","            X_test.append(sentence)\n","    sequences = tokenizer.texts_to_sequences(X_test)\n","    X_test = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n","    print('Preparing embedding matrix. - 2')\n","\n","    word_index = tokenizer.word_index\n","    num_words = len(word_index) + 1\n","    embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n","    for word, i in word_index.items():\n","        embedding_vector = embeddings_index.get(word)\n","        if embedding_vector is not None:\n","            embedding_matrix[i] = embedding_vector\n","    embedding_layer = Embedding(num_words,\n","                                EMBEDDING_DIM,\n","                                weights=[embedding_matrix],\n","                                input_length=MAX_SEQUENCE_LENGTH,\n","                                trainable=True)\n","    sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH, ), dtype='int32')\n","    embedded_sequences = embedding_layer(sequence_input)\n","\n","    print(\"Start training...\")\n","    model = RNN_model(embedded_sequences, num_class)\n","    model.compile(loss='categorical_crossentropy', optimizer=keras.optimizers.Adam(learning_rate=0.001, clipvalue=15), metrics=['accuracy'])\n","    early_stop = EarlyStopping(monitor='val_loss' if VALIDATION_SPLIT > 0 else \"loss\", patience=15, mode='min')\n","    model.fit(X_train, Y_train, batch_size=128, epochs=50, callbacks=[early_stop], validation_data=(X_val, Y_val) if VALIDATION_SPLIT > 0 else None)\n","    Y_pre = model.predict(X_test)\n","    Y_pre = np.argmax(Y_pre, axis=1)\n","    Y_pre = [y_to_label[i] for i in Y_pre]\n","    with open(\"/content/drive/My Drive/Colab Notebooks/NLP_FS23_proj/predict.txt\", 'w') as f:\n","        for ID, label in zip(ID_test, Y_pre):\n","            f.write(ID + \"\\t\" + label + \"\\n\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XS2MOGv9cjAF","executionInfo":{"status":"ok","timestamp":1698802684490,"user_tz":240,"elapsed":539235,"user":{"displayName":"sivaram learning","userId":"00911034967220677585"}},"outputId":"373cc7c6-c247-49e6-a634-4c7fcb8deb89"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 400000 word vectors.\n","Preparing embedding matrix.\n","Preparing embedding matrix. - 2\n","Start training...\n","Model: \"model\"\n","__________________________________________________________________________________________________\n"," Layer (type)                Output Shape                 Param #   Connected to                  \n","==================================================================================================\n"," input_1 (InputLayer)        [(None, 100)]                0         []                            \n","                                                                                                  \n"," embedding (Embedding)       (None, 100, 50)              978150    ['input_1[0][0]']             \n","                                                                                                  \n"," bidirectional (Bidirection  (None, 2048)                 8806400   ['embedding[0][0]']           \n"," al)                                                                                              \n","                                                                                                  \n"," bidirectional_1 (Bidirecti  (None, 100, 2048)            8806400   ['embedding[0][0]']           \n"," onal)                                                                                            \n","                                                                                                  \n"," dot (Dot)                   (None, 100)                  0         ['bidirectional[0][0]',       \n","                                                                     'bidirectional_1[0][0]']     \n","                                                                                                  \n"," lambda (Lambda)             (None, 100)                  0         ['dot[0][0]']                 \n","                                                                                                  \n"," repeat_vector (RepeatVecto  (None, 2048, 100)            0         ['lambda[0][0]']              \n"," r)                                                                                               \n","                                                                                                  \n"," permute (Permute)           (None, 100, 2048)            0         ['repeat_vector[0][0]']       \n","                                                                                                  \n"," multiply (Multiply)         (None, 100, 2048)            0         ['permute[0][0]',             \n","                                                                     'bidirectional_1[0][0]']     \n","                                                                                                  \n"," lambda_1 (Lambda)           (None, 2048)                 0         ['multiply[0][0]']            \n","                                                                                                  \n"," dense (Dense)               (None, 512)                  1049088   ['lambda_1[0][0]']            \n","                                                                                                  \n"," batch_normalization (Batch  (None, 512)                  2048      ['dense[0][0]']               \n"," Normalization)                                                                                   \n","                                                                                                  \n"," activation (Activation)     (None, 512)                  0         ['batch_normalization[0][0]'] \n","                                                                                                  \n"," dense_1 (Dense)             (None, 256)                  131328    ['activation[0][0]']          \n","                                                                                                  \n"," batch_normalization_1 (Bat  (None, 256)                  1024      ['dense_1[0][0]']             \n"," chNormalization)                                                                                 \n","                                                                                                  \n"," activation_1 (Activation)   (None, 256)                  0         ['batch_normalization_1[0][0]'\n","                                                                    ]                             \n","                                                                                                  \n"," dropout (Dropout)           (None, 256)                  0         ['activation_1[0][0]']        \n","                                                                                                  \n"," dense_2 (Dense)             (None, 19)                   4883      ['dropout[0][0]']             \n","                                                                                                  \n","==================================================================================================\n","Total params: 19779321 (75.45 MB)\n","Trainable params: 19777785 (75.45 MB)\n","Non-trainable params: 1536 (6.00 KB)\n","__________________________________________________________________________________________________\n","None\n","Epoch 1/50\n","57/57 [==============================] - 45s 474ms/step - loss: 2.5597 - accuracy: 0.2438 - val_loss: 3.0151 - val_accuracy: 0.0763\n","Epoch 2/50\n","57/57 [==============================] - 26s 456ms/step - loss: 2.0606 - accuracy: 0.3464 - val_loss: 2.7601 - val_accuracy: 0.1525\n","Epoch 3/50\n","57/57 [==============================] - 27s 479ms/step - loss: 1.7632 - accuracy: 0.4353 - val_loss: 3.1966 - val_accuracy: 0.1050\n","Epoch 4/50\n","57/57 [==============================] - 28s 487ms/step - loss: 1.4098 - accuracy: 0.5451 - val_loss: 3.0682 - val_accuracy: 0.1538\n","Epoch 5/50\n","57/57 [==============================] - 27s 480ms/step - loss: 1.1611 - accuracy: 0.6236 - val_loss: 3.4540 - val_accuracy: 0.1550\n","Epoch 6/50\n","57/57 [==============================] - 27s 479ms/step - loss: 0.9797 - accuracy: 0.6831 - val_loss: 4.0459 - val_accuracy: 0.0962\n","Epoch 7/50\n","57/57 [==============================] - 27s 480ms/step - loss: 0.8391 - accuracy: 0.7308 - val_loss: 4.6406 - val_accuracy: 0.2050\n","Epoch 8/50\n","57/57 [==============================] - 28s 485ms/step - loss: 0.6934 - accuracy: 0.7788 - val_loss: 4.4912 - val_accuracy: 0.1912\n","Epoch 9/50\n","57/57 [==============================] - 28s 485ms/step - loss: 0.5916 - accuracy: 0.8163 - val_loss: 5.6953 - val_accuracy: 0.2087\n","Epoch 10/50\n","57/57 [==============================] - 28s 485ms/step - loss: 0.5013 - accuracy: 0.8485 - val_loss: 7.8102 - val_accuracy: 0.0487\n","Epoch 11/50\n","57/57 [==============================] - 27s 480ms/step - loss: 0.3960 - accuracy: 0.8833 - val_loss: 9.3979 - val_accuracy: 0.1762\n","Epoch 12/50\n","57/57 [==============================] - 27s 480ms/step - loss: 0.3171 - accuracy: 0.9090 - val_loss: 10.5269 - val_accuracy: 0.1688\n","Epoch 13/50\n","57/57 [==============================] - 27s 477ms/step - loss: 0.2630 - accuracy: 0.9257 - val_loss: 10.6912 - val_accuracy: 0.1225\n","Epoch 14/50\n","57/57 [==============================] - 27s 480ms/step - loss: 0.2121 - accuracy: 0.9457 - val_loss: 20.0080 - val_accuracy: 0.0613\n","Epoch 15/50\n","57/57 [==============================] - 28s 489ms/step - loss: 0.1609 - accuracy: 0.9611 - val_loss: 16.2888 - val_accuracy: 0.1950\n","Epoch 16/50\n","57/57 [==============================] - 28s 485ms/step - loss: 0.1217 - accuracy: 0.9757 - val_loss: 15.5934 - val_accuracy: 0.1462\n","Epoch 17/50\n","57/57 [==============================] - 28s 485ms/step - loss: 0.0998 - accuracy: 0.9828 - val_loss: 27.4208 - val_accuracy: 0.0575\n","85/85 [==============================] - 6s 59ms/step\n"]}]},{"cell_type":"code","source":["import json\n","import sys\n","import re\n","import numpy as np\n","import tensorflow as tf\n","import random as rn\n","import keras\n","import os\n","from keras.layers import Embedding, LSTM, Conv1D, BatchNormalization, Multiply, Permute, Dot\n","from keras.layers import Dropout, MaxPooling1D, GlobalMaxPooling1D, Lambda, RepeatVector\n","from keras.layers import Input, Activation, Bidirectional, Dense\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.utils import to_categorical\n","from keras.preprocessing.text import Tokenizer\n","from keras.models import Model\n","from keras.callbacks import EarlyStopping\n","from keras import backend as K\n","from sklearn.metrics import f1_score, confusion_matrix, mean_squared_error\n","\n","np.random.seed(42)\n","rn.seed(12345)\n","\n","EMBEDDING_DIM = 50  # word embedding size\n","MAX_TEXT_LENGTH = 100  # Word embedding size\n","VALIDATION_SPLIT_RATIO = 0.1  # Ratio of validation data\n","\n","def create_custom_model(input_layer, num_target_classes):\n","    def apply_smoothing_attention(x):\n","        e = K.sigmoid(x)\n","        s = K.sum(e, axis=-1, keepdims=True)\n","        return e / s\n","\n","    regularization_strength = 0.0001\n","    dropout_rate = 0.5\n","    hidden_dimension = 1024\n","\n","    vector = Bidirectional(LSTM(hidden_dimension, return_sequences=False, kernel_regularizer=keras.regularizers.l2(regularization_strength)))(input_layer)\n","    lstm = Bidirectional(LSTM(hidden_dimension, return_sequences=True, kernel_regularizer=keras.regularizers.l2(regularization_strength)))(input_layer)\n","    cosine_similarity = Dot(axes=-1, normalize=True)([vector, lstm])\n","    attention_weights = Lambda(apply_smoothing_attention)(cosine_similarity)\n","    attention_weights = RepeatVector(2*hidden_dimension)(attention_weights)\n","    attention_weights = Permute((2, 1))(attention_weights)\n","    output = Multiply()([attention_weights, lstm])\n","    output = Lambda(lambda x: K.sum(x, axis=1))(output)\n","    output = Dense(512)(output)\n","    output = BatchNormalization()(output)\n","    output = Activation(\"relu\")(output)\n","    output = Dense(256)(output)\n","    output = BatchNormalization()(output)\n","    output = Activation(\"relu\")(output)\n","    output = Dropout(dropout_rate)(output)\n","    output = Dense(num_target_classes, activation='softmax')(output)\n","    model = Model(sequence_input, output)\n","    print(model.summary())\n","    return model\n","\n","if __name__ == \"__main__\":\n","    word_embeddings = {}\n","    with open('/content/drive/My Drive/Colab Notebooks/NLP_FS23_proj/glove.6B.50d.txt', encoding=\"utf-8\", errors=\"ignore\") as file:\n","        for line in file:\n","            values = line.split()\n","            word = values[0]\n","            vector = np.asarray(values[1:], dtype='float32')\n","            word_embeddings[word] = vector\n","    print('Found %s word vectors.' % len(word_embeddings))\n","    input_text = []\n","    target_labels = []\n","    label_to_index = dict()\n","\n","    with open(\"/content/drive/My Drive/Colab Notebooks/NLP_FS23_proj/TRAIN_FILE.txt\") as file:\n","        for index, line in enumerate(file):\n","            line = line.strip()\n","            if index % 4 == 0:\n","                ID, sentence = line.split(\"\\t\")\n","                sentence = sentence[1:-1]\n","                sentence = sentence.replace('<e1>', 'xxxxxxxxxe1xxxxxxxxx ')\n","                sentence = sentence.replace('<e2>', 'xxxxxxxxxe2xxxxxxxxx ')\n","                sentence = sentence.replace('</e1>', ' sssssssmse1sssssssss')\n","                sentence = sentence.replace('</e2>', ' sssssssmse2sssssssss')\n","                input_text.append(sentence)\n","            elif index % 4 == 1:\n","                label = line\n","                if label not in label_to_index:\n","                    label_to_index[label] = len(label_to_index)\n","                target_labels.append(label_to_index[label])\n","\n","    index_to_label = {index: label for label, index in label_to_index.items()}\n","    target_labels = np.array(target_labels, dtype=int)\n","    num_target_classes = max(target_labels) + 1\n","    target_labels = to_categorical(target_labels)\n","\n","    tokenizer = Tokenizer(oov_token=\"UNK\")\n","    tokenizer.fit_on_texts(input_text)\n","    sequences = tokenizer.texts_to_sequences(input_text)\n","    input_text = pad_sequences(sequences, maxlen=MAX_TEXT_LENGTH)\n","    print('Preparing embedding matrix.')\n","\n","    if VALIDATION_SPLIT_RATIO > 0:\n","        indices = np.arange(len(target_labels))\n","        np.random.shuffle(indices)\n","        validation_index = int(VALIDATION_SPLIT_RATIO * len(target_labels))\n","        validation_input = input_text[indices[:validation_index]]\n","        validation_labels = target_labels[indices[:validation_index]]\n","        input_text = input_text[indices[validation_index:]]\n","        target_labels = target_labels[indices[validation_index:]]\n","\n","    test_input = []\n","    test_IDs = []\n","\n","    with open(\"/content/drive/My Drive/Colab Notebooks/NLP_FS23_proj/TEST_FILE.txt\") as file:\n","        for line in file:\n","            ID, sentence = line.strip().split(\"\\t\")\n","            sentence = sentence[1:-1]\n","            sentence = sentence.replace('<e1>', 'xxxxxxxxxe1xxxxxxxxx ')\n","            sentence = sentence.replace('<e2>', 'xxxxxxxxxe2xxxxxxxxx ')\n","            sentence = sentence.replace('</e1>', ' sssssssmse1sssssssss')\n","            sentence = sentence.replace('</e2>', ' sssssssmse2sssssssss')\n","            test_IDs.append(ID)\n","            test_input.append(sentence)\n","\n","    sequences = tokenizer.texts_to_sequences(test_input)\n","    test_input = pad_sequences(sequences, maxlen=MAX_TEXT_LENGTH)\n","    print('Preparing embedding matrix. - 2')\n","\n","    word_index = tokenizer.word_index\n","    num_words = len(word_index) + 1\n","    word_embedding_matrix = np.zeros((num_words, EMBEDDING_SIZE))\n","\n","    for word, index in word_index.items():\n","        word_vector = word_embeddings.get(word)\n","        if word_vector is not None:\n","            word_embedding_matrix[index] = word_vector\n","\n","    embedding_layer = Embedding(num_words, EMBEDDING_SIZE, weights=[word_embedding_matrix], input_length=MAX_TEXT_LENGTH, trainable=True)\n","    sequence_input = Input(shape=(MAX_TEXT_LENGTH, ), dtype='int32')\n","    embedded_sequences = embedding_layer(sequence_input)\n","\n","    print(\"Start training...\")\n","    model = create_custom_model(embedded_sequences, num_target_classes)\n","    model.compile(loss='categorical_crossentropy', optimizer=keras.optimizers.Adam(learning_rate=0.001, clipvalue=15), metrics=['accuracy'])\n","\n","    early_stop = EarlyStopping(monitor='val_loss' if VALIDATION_SPLIT_RATIO > 0 else \"loss\", patience=15, mode='min')\n","\n","    model.fit(input_text, target_labels, batch_size=128, epochs=50, validation_data=(validation_input, validation_labels) if VALIDATION_SPLIT_RATIO > 0 else None)\n","\n","    predictions = model.predict(test_input)\n","    predicted_labels = np.argmax(predictions, axis=1)\n","    predicted_labels = [index_to_label[index] for index in predicted_labels]\n","\n","    with open(\"/content/drive/My Drive/Colab Notebooks/NLP_FS23_proj/predict.txt\", 'w') as file:\n","        for ID, label in zip(test_IDs, predicted_labels):\n","            file.write(ID + \"\\t\" + label + \"\\n\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"N8PRdWKti0AN","executionInfo":{"status":"ok","timestamp":1698805628611,"user_tz":240,"elapsed":1399728,"user":{"displayName":"sivaram learning","userId":"00911034967220677585"}},"outputId":"76cf1e87-f8ae-4425-a61d-90c07552236b"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 400000 word vectors.\n","Preparing embedding matrix.\n","Preparing embedding matrix. - 2\n","Start training...\n","Model: \"model_3\"\n","__________________________________________________________________________________________________\n"," Layer (type)                Output Shape                 Param #   Connected to                  \n","==================================================================================================\n"," input_4 (InputLayer)        [(None, 100)]                0         []                            \n","                                                                                                  \n"," embedding_3 (Embedding)     (None, 100, 50)              978150    ['input_4[0][0]']             \n","                                                                                                  \n"," bidirectional_6 (Bidirecti  (None, 2048)                 8806400   ['embedding_3[0][0]']         \n"," onal)                                                                                            \n","                                                                                                  \n"," bidirectional_7 (Bidirecti  (None, 100, 2048)            8806400   ['embedding_3[0][0]']         \n"," onal)                                                                                            \n","                                                                                                  \n"," dot_3 (Dot)                 (None, 100)                  0         ['bidirectional_6[0][0]',     \n","                                                                     'bidirectional_7[0][0]']     \n","                                                                                                  \n"," lambda_6 (Lambda)           (None, 100)                  0         ['dot_3[0][0]']               \n","                                                                                                  \n"," repeat_vector_3 (RepeatVec  (None, 2048, 100)            0         ['lambda_6[0][0]']            \n"," tor)                                                                                             \n","                                                                                                  \n"," permute_3 (Permute)         (None, 100, 2048)            0         ['repeat_vector_3[0][0]']     \n","                                                                                                  \n"," multiply_3 (Multiply)       (None, 100, 2048)            0         ['permute_3[0][0]',           \n","                                                                     'bidirectional_7[0][0]']     \n","                                                                                                  \n"," lambda_7 (Lambda)           (None, 2048)                 0         ['multiply_3[0][0]']          \n","                                                                                                  \n"," dense_9 (Dense)             (None, 512)                  1049088   ['lambda_7[0][0]']            \n","                                                                                                  \n"," batch_normalization_6 (Bat  (None, 512)                  2048      ['dense_9[0][0]']             \n"," chNormalization)                                                                                 \n","                                                                                                  \n"," activation_6 (Activation)   (None, 512)                  0         ['batch_normalization_6[0][0]'\n","                                                                    ]                             \n","                                                                                                  \n"," dense_10 (Dense)            (None, 256)                  131328    ['activation_6[0][0]']        \n","                                                                                                  \n"," batch_normalization_7 (Bat  (None, 256)                  1024      ['dense_10[0][0]']            \n"," chNormalization)                                                                                 \n","                                                                                                  \n"," activation_7 (Activation)   (None, 256)                  0         ['batch_normalization_7[0][0]'\n","                                                                    ]                             \n","                                                                                                  \n"," dropout_3 (Dropout)         (None, 256)                  0         ['activation_7[0][0]']        \n","                                                                                                  \n"," dense_11 (Dense)            (None, 19)                   4883      ['dropout_3[0][0]']           \n","                                                                                                  \n","==================================================================================================\n","Total params: 19779321 (75.45 MB)\n","Trainable params: 19777785 (75.45 MB)\n","Non-trainable params: 1536 (6.00 KB)\n","__________________________________________________________________________________________________\n","None\n","Epoch 1/50\n","57/57 [==============================] - 37s 494ms/step - loss: 2.5093 - accuracy: 0.2619 - val_loss: 3.1179 - val_accuracy: 0.1037\n","Epoch 2/50\n","57/57 [==============================] - 27s 469ms/step - loss: 2.0220 - accuracy: 0.3575 - val_loss: 2.9070 - val_accuracy: 0.0975\n","Epoch 3/50\n","57/57 [==============================] - 27s 476ms/step - loss: 1.7115 - accuracy: 0.4460 - val_loss: 2.7553 - val_accuracy: 0.1688\n","Epoch 4/50\n","57/57 [==============================] - 27s 473ms/step - loss: 1.3847 - accuracy: 0.5481 - val_loss: 2.6351 - val_accuracy: 0.2288\n","Epoch 5/50\n","57/57 [==============================] - 27s 479ms/step - loss: 1.1680 - accuracy: 0.6244 - val_loss: 2.7484 - val_accuracy: 0.1737\n","Epoch 6/50\n","57/57 [==============================] - 28s 485ms/step - loss: 0.9883 - accuracy: 0.6793 - val_loss: 2.2190 - val_accuracy: 0.2725\n","Epoch 7/50\n","57/57 [==============================] - 27s 482ms/step - loss: 0.8307 - accuracy: 0.7396 - val_loss: 2.1610 - val_accuracy: 0.2562\n","Epoch 8/50\n","57/57 [==============================] - 27s 479ms/step - loss: 0.7210 - accuracy: 0.7744 - val_loss: 2.0919 - val_accuracy: 0.3313\n","Epoch 9/50\n","57/57 [==============================] - 27s 479ms/step - loss: 0.6039 - accuracy: 0.8129 - val_loss: 2.3409 - val_accuracy: 0.3338\n","Epoch 10/50\n","57/57 [==============================] - 27s 482ms/step - loss: 0.4984 - accuracy: 0.8468 - val_loss: 1.4661 - val_accuracy: 0.5537\n","Epoch 11/50\n","57/57 [==============================] - 28s 484ms/step - loss: 0.4101 - accuracy: 0.8782 - val_loss: 2.8768 - val_accuracy: 0.3425\n","Epoch 12/50\n","57/57 [==============================] - 27s 482ms/step - loss: 0.3090 - accuracy: 0.9142 - val_loss: 1.5644 - val_accuracy: 0.5300\n","Epoch 13/50\n","57/57 [==============================] - 27s 481ms/step - loss: 0.2556 - accuracy: 0.9318 - val_loss: 2.2116 - val_accuracy: 0.3413\n","Epoch 14/50\n","57/57 [==============================] - 28s 484ms/step - loss: 0.2225 - accuracy: 0.9438 - val_loss: 1.4252 - val_accuracy: 0.6100\n","Epoch 15/50\n","57/57 [==============================] - 28s 484ms/step - loss: 0.1702 - accuracy: 0.9601 - val_loss: 2.0238 - val_accuracy: 0.5225\n","Epoch 16/50\n","57/57 [==============================] - 27s 482ms/step - loss: 0.1392 - accuracy: 0.9693 - val_loss: 1.8602 - val_accuracy: 0.5562\n","Epoch 17/50\n","57/57 [==============================] - 27s 482ms/step - loss: 0.1111 - accuracy: 0.9783 - val_loss: 3.0907 - val_accuracy: 0.4475\n","Epoch 18/50\n","57/57 [==============================] - 27s 479ms/step - loss: 0.1085 - accuracy: 0.9783 - val_loss: 2.0052 - val_accuracy: 0.5800\n","Epoch 19/50\n","57/57 [==============================] - 27s 476ms/step - loss: 0.0887 - accuracy: 0.9849 - val_loss: 1.8633 - val_accuracy: 0.5900\n","Epoch 20/50\n","57/57 [==============================] - 28s 486ms/step - loss: 0.0664 - accuracy: 0.9914 - val_loss: 1.8033 - val_accuracy: 0.6150\n","Epoch 21/50\n","57/57 [==============================] - 28s 484ms/step - loss: 0.0679 - accuracy: 0.9894 - val_loss: 2.0143 - val_accuracy: 0.5713\n","Epoch 22/50\n","57/57 [==============================] - 27s 483ms/step - loss: 0.0580 - accuracy: 0.9926 - val_loss: 1.8728 - val_accuracy: 0.6225\n","Epoch 23/50\n","57/57 [==============================] - 28s 484ms/step - loss: 0.0615 - accuracy: 0.9917 - val_loss: 2.1977 - val_accuracy: 0.5462\n","Epoch 24/50\n","57/57 [==============================] - 27s 481ms/step - loss: 0.0628 - accuracy: 0.9903 - val_loss: 2.8877 - val_accuracy: 0.4363\n","Epoch 25/50\n","57/57 [==============================] - 28s 485ms/step - loss: 0.0512 - accuracy: 0.9931 - val_loss: 2.7751 - val_accuracy: 0.5188\n","Epoch 26/50\n","57/57 [==============================] - 27s 482ms/step - loss: 0.0593 - accuracy: 0.9897 - val_loss: 2.7223 - val_accuracy: 0.4625\n","Epoch 27/50\n","57/57 [==============================] - 27s 482ms/step - loss: 0.0709 - accuracy: 0.9890 - val_loss: 4.0786 - val_accuracy: 0.3100\n","Epoch 28/50\n","57/57 [==============================] - 27s 479ms/step - loss: 0.0555 - accuracy: 0.9915 - val_loss: 2.7313 - val_accuracy: 0.4938\n","Epoch 29/50\n","57/57 [==============================] - 28s 484ms/step - loss: 0.0559 - accuracy: 0.9912 - val_loss: 2.4495 - val_accuracy: 0.5138\n","Epoch 30/50\n","57/57 [==============================] - 27s 482ms/step - loss: 0.0437 - accuracy: 0.9946 - val_loss: 2.3192 - val_accuracy: 0.5200\n","Epoch 31/50\n","57/57 [==============================] - 28s 486ms/step - loss: 0.0386 - accuracy: 0.9961 - val_loss: 2.5634 - val_accuracy: 0.5288\n","Epoch 32/50\n","57/57 [==============================] - 28s 485ms/step - loss: 0.0468 - accuracy: 0.9932 - val_loss: 2.0133 - val_accuracy: 0.5838\n","Epoch 33/50\n","57/57 [==============================] - 27s 477ms/step - loss: 0.0390 - accuracy: 0.9964 - val_loss: 2.8289 - val_accuracy: 0.5113\n","Epoch 34/50\n","57/57 [==============================] - 28s 484ms/step - loss: 0.0288 - accuracy: 0.9992 - val_loss: 2.3204 - val_accuracy: 0.5725\n","Epoch 35/50\n","57/57 [==============================] - 27s 477ms/step - loss: 0.0254 - accuracy: 0.9989 - val_loss: 3.0346 - val_accuracy: 0.5312\n","Epoch 36/50\n","57/57 [==============================] - 28s 485ms/step - loss: 0.0284 - accuracy: 0.9978 - val_loss: 2.3692 - val_accuracy: 0.6075\n","Epoch 37/50\n","57/57 [==============================] - 28s 487ms/step - loss: 0.0531 - accuracy: 0.9918 - val_loss: 3.3767 - val_accuracy: 0.5013\n","Epoch 38/50\n","57/57 [==============================] - 27s 479ms/step - loss: 0.0449 - accuracy: 0.9933 - val_loss: 4.0586 - val_accuracy: 0.4487\n","Epoch 39/50\n","57/57 [==============================] - 28s 486ms/step - loss: 0.0533 - accuracy: 0.9897 - val_loss: 6.3361 - val_accuracy: 0.3013\n","Epoch 40/50\n","57/57 [==============================] - 27s 476ms/step - loss: 0.0561 - accuracy: 0.9886 - val_loss: 3.2679 - val_accuracy: 0.4688\n","Epoch 41/50\n","57/57 [==============================] - 28s 484ms/step - loss: 0.0582 - accuracy: 0.9885 - val_loss: 2.2292 - val_accuracy: 0.5925\n","Epoch 42/50\n","57/57 [==============================] - 27s 478ms/step - loss: 0.0501 - accuracy: 0.9911 - val_loss: 4.3893 - val_accuracy: 0.3688\n","Epoch 43/50\n","57/57 [==============================] - 27s 478ms/step - loss: 0.0408 - accuracy: 0.9947 - val_loss: 3.3113 - val_accuracy: 0.5200\n","Epoch 44/50\n","57/57 [==============================] - 28s 484ms/step - loss: 0.0426 - accuracy: 0.9946 - val_loss: 2.7465 - val_accuracy: 0.5150\n","Epoch 45/50\n","57/57 [==============================] - 27s 482ms/step - loss: 0.0318 - accuracy: 0.9968 - val_loss: 2.0514 - val_accuracy: 0.6200\n","Epoch 46/50\n","57/57 [==============================] - 28s 483ms/step - loss: 0.0369 - accuracy: 0.9956 - val_loss: 2.8477 - val_accuracy: 0.4775\n","Epoch 47/50\n","57/57 [==============================] - 27s 481ms/step - loss: 0.0299 - accuracy: 0.9971 - val_loss: 2.6106 - val_accuracy: 0.5625\n","Epoch 48/50\n","57/57 [==============================] - 28s 486ms/step - loss: 0.0407 - accuracy: 0.9949 - val_loss: 2.9131 - val_accuracy: 0.4938\n","Epoch 49/50\n","57/57 [==============================] - 28s 486ms/step - loss: 0.0651 - accuracy: 0.9850 - val_loss: 4.9268 - val_accuracy: 0.3537\n","Epoch 50/50\n","57/57 [==============================] - 27s 478ms/step - loss: 0.0423 - accuracy: 0.9937 - val_loss: 2.5557 - val_accuracy: 0.5537\n","85/85 [==============================] - 6s 58ms/step\n"]}]}]}